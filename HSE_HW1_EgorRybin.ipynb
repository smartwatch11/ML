{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNVbHNvuX/bA0jGfUWPyhW0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smartwatch11/ML/blob/main/HSE_HW1_EgorRybin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the necessary libraries\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.optimize as opt\n",
        "import sklearn.linear_model\n",
        "import sklearn.model_selection\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "#download dataset\n",
        "data = fetch_20newsgroups() \n",
        "\n",
        "#creating an array of penalties to run all 3 variants of penalty (regularization) in one loop\n",
        "penalties = ['l1', 'l2', 'elasticnet']\n",
        "\n",
        "#loop over penalties\n",
        "for penalty in penalties:\n",
        "    \n",
        "    #creating a model for training\n",
        "    #use linear model with Stochastic Gradient Descent Classifier\n",
        "    #the loss function is log\n",
        "    model = sklearn.linear_model.SGDClassifier(loss='log', penalty=penalty)\n",
        "    \n",
        "    #creating a model for selection of dataset for train and test\n",
        "    #n_splits - Number of folds; shuffle - shuffle the data before splitting into batches; random_state - control the randomness of each fold\n",
        "    xval = sklearn.model_selection.KFold(n_splits=3, shuffle=True, random_state=7)\n",
        "    \n",
        "    #variable for step count of n_splits\n",
        "    step = 0\n",
        "    \n",
        "    #splitting the dataset into training and test parts\n",
        "    for train, test in xval.split(data['data']):\n",
        "        #print(train, test)\n",
        "        step = step + 1\n",
        "        \n",
        "        #source array for the training part\n",
        "        X_train = []\n",
        "        #target array for the training part\n",
        "        Y_train = []\n",
        "        for i in train:\n",
        "            X_train.append(data['data'][i])\n",
        "            Y_train.append(data['target_names'][data['target'][i]])\n",
        "        \n",
        "        \n",
        "        #source array for the test part\n",
        "        X_test = []\n",
        "        #target array for the test part so that after predicting the model, the accuracy of the model can be calculated\n",
        "        Y_test = []\n",
        "        for i in test:\n",
        "            X_test.append(data['data'][i])\n",
        "            Y_test.append(data['target_names'][data['target'][i]])\n",
        "        \n",
        "        #encoding the text into sparse features in training part\n",
        "        #n_features - he number of features (columns) in the output matrices, in our case a number of words in English (let's say 10000000)\n",
        "        #binary - If True, all non zero counts are set to 1; norm - None for no normalization of term vectors\n",
        "        vectorizer = HashingVectorizer(n_features=1000000, binary=True, norm=None)\n",
        "        train_vect = vectorizer.fit_transform(X_train)\n",
        "    \n",
        "        #model training\n",
        "        model.fit(train_vect, np.ravel(Y_train))\n",
        "        #print(model.coef_ )   \n",
        "        \n",
        "        #encoding the text into sparse features in test part\n",
        "        test_vect = vectorizer.fit_transform(X_test)\n",
        "        \n",
        "        #run the model on the test part and check with the target class\n",
        "        count = 0\n",
        "        for i in range(len(Y_test)):\n",
        "            if model.predict(test_vect[i]) == Y_test[i]:\n",
        "                count = count + 1\n",
        "        \n",
        "        #output the prediction accuracy of our trained model as a percentage\n",
        "        print('Accuracy of penalty', penalty, 'after', step, 'iteration', '=', round((count/len(Y_test))*100, 2),'%')\n",
        "    \n",
        "    print('\\n')\n",
        "    #clear model variables for next penalty\n",
        "    del model\n",
        "    del xval"
      ],
      "metadata": {
        "id": "hLvDjC1eZp-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456b24d6-581e-4549-ec2b-9efccbff2e02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of penalty l1 after 1 iteration = 81.52 %\n",
            "Accuracy of penalty l1 after 2 iteration = 79.98 %\n",
            "Accuracy of penalty l1 after 3 iteration = 80.43 %\n",
            "\n",
            "\n",
            "Accuracy of penalty l2 after 1 iteration = 87.35 %\n",
            "Accuracy of penalty l2 after 2 iteration = 86.48 %\n",
            "Accuracy of penalty l2 after 3 iteration = 87.51 %\n",
            "\n",
            "\n",
            "Accuracy of penalty elasticnet after 1 iteration = 86.48 %\n",
            "Accuracy of penalty elasticnet after 2 iteration = 85.44 %\n",
            "Accuracy of penalty elasticnet after 3 iteration = 86.37 %\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The best model is l2**, since L2 regularization, while calculating the loss function in the gradient calculation step, the loss function tries to minimize the loss by subtracting it from the average of the data distribution.\n",
        "\n",
        "The main intuitive difference between the L1 and L2 regularization is that L1 regularization tries to estimate the median of the data while the L2 regularization tries to estimate the mean of the data to avoid overfitting."
      ],
      "metadata": {
        "id": "JNQGzoSdCPWB"
      }
    }
  ]
}